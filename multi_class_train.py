
from keras.callbacks import CSVLogger
from keras.layers import Dense
from keras.models import Model
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from get_dataset import get_df_1
from get_dataset import get_df_2
from Parser import parameter_parser
#from models.cnn import get_cnn

import numpy as np
from sklearn.metrics import confusion_matrix
import os
from BoW import get_df_bow
from TF_IDF import get_df_tfidf

args = parameter_parser()
num_classes = 7
from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, Input, Dropout
from Parser import parameter_parser
from keras.models import Sequential
from keras.models import Model
args = parameter_parser()




from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

from imblearn.over_sampling import SMOTE

def get_train_test(dataset):
    vectors = np.stack(dataset.iloc[:, 0].values)
    labels = dataset.iloc[:, 1].values
    print("Shape of vectors before reshape:", vectors.shape)  # Print shape before reshape
    
    ## Reshape Word2Vec vectors to 2D because the word2vec is 3D
    vectors = vectors.reshape(vectors.shape[0], -1)
    # Keep only the first 300 elements of each vector
    # vectors= vectors[:, :300]

    print("Shape of vectors After reshape:", vectors.shape)

    # Apply SMOTE to balance all classes
    smote = SMOTE(sampling_strategy='auto', random_state=42)
    vectors_resampled, labels_resampled = smote.fit_resample(vectors, labels)

    # Apply random under-sampling to ensure balance
    #applies SMOTE to oversample all classes and then applies random under-sampling to ensure that all classes are balanced. By balancing all classes, 
    #you ensure that the model is trained on a more representative dataset, which can improve its performance across all classes.
    undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)
    vectors_resampled, labels_resampled = undersampler.fit_resample(vectors_resampled, labels_resampled)

    # Split the data into train and test sets
    x_train, x_test, y_train, y_test = train_test_split(vectors_resampled, labels_resampled, test_size=0.2, random_state=42,
                                                        stratify=labels_resampled)

    return x_train, x_test, y_train, y_test


# Function to define the model architecture
def ClassiFilerNet(INPUT_SIZE, TIME_STEPS, model_name):
    model = Sequential()
    if model_name in ['Word2Vec', 'FastText']:
        batch_input_shape = (None,1, 100, 300)  # Adjusted for 2D input
    elif model_name == 'BoW':
        batch_input_shape = (None, 1,37, 1)  # Adjusted for 2D input
    elif model_name == 'TF-IDF':
        batch_input_shape = (None,1, 300, 1)  # Adjusted for 2D input
    else:
        batch_input_shape = (None, *INPUT_SIZE)

    model.add(
        Convolution2D(
            filters=32,
            kernel_size=(3, 3),  # Adjusted for 2D kernel, tells us the size of this filter. So, it's a 3x3 filter.
            padding="same",
            input_shape=batch_input_shape[1:],  # Remove the batch dimension
            name='cnn_input'
        )
    )
    #Regarding the kernel size (the (3, 3) part), it determines how big the filter is that scans over our input data. Smaller filters look for small features, and larger filters look for larger features. A (3, 3) filter is a common choice because it's small enough to capture fine details while also being computationally efficient.
    model.add(Activation('relu'))
    model.add(MaxPooling2D(
        pool_size=(2, 2),
        strides=(2, 2),
        padding='same',
    ))
    model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))  # Adjusted for 2D kernel
    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
    model.add(Flatten())
    model.add(Dropout(0.5))  # Add dropout layer with dropout rate 0.5
    model.add(Dense(300, activation='relu', name='dense_fc1'))
    model.add(Dense(300, activation='relu', name='dense_fc2'))
    model.add(Dense(num_classes, activation='softmax', name='dense_output'))  # Change activation to softmax

    return model


def train():
    dataset_name = os.path.splitext(os.path.basename(args.dataset))[0]
    log_directory = os.path.join("log", f"{dataset_name}_logs")
    os.makedirs(log_directory, exist_ok=True)
    
    train_models = [
        {'name': 'Word2Vec', 'get_data_function': get_df_1},
        {'name': 'FastText', 'get_data_function': get_df_2},
        {'name': 'BoW', 'get_data_function': get_df_bow},
       {'name': 'TF-IDF', 'get_data_function': get_df_tfidf}
    ]
    
    for model_info in train_models:
        print(f"Training for {model_info['name']}")
        
        df, _ = model_info['get_data_function']()
        x_train, x_test, y_train, y_test = get_train_test(df)
        
        print("Shape of x_train:", x_train.shape)# shape is  (50361, 300) for BoW and TF-IFD and  word2vec Shape of vectors before reshape: (15174, 100, 300)

        if model_info['name'] == 'TF-IDF':
            x_train = x_train.reshape(-1, 1, 300, 1)#133, 105
            x_test = x_test.reshape(-1, 1, 300, 1)##133, 105
        elif model_info['name'] == 'BoW':
            x_train = x_train.reshape((-1, 1, 37, 1))#37
            x_test = x_test.reshape((-1, 1, 37, 1))#37
        else:
            x_train = x_train.reshape(-1, 1, 100, 300)#133
            x_test = x_test.reshape(-1, 1, 100, 300)#133
            # Construct the model
        
        # No One-Hot Encoding
        y_train_encoded = y_train 
        y_test_encoded = y_test   # Adjust label values to start from 0

        # Training
        model = ClassiFilerNet(x_train.shape[1], x_train.shape[2], model_info['name'])
        
        adam = Adam(learning_rate=args.lr)

        model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

        log_file_path = os.path.join(log_directory, f'{model_info["name"]}_log.txt')
        csv_logger = CSVLogger(log_file_path, append=True, separator=',')

        history = model.fit(
            x=x_train,
            y=y_train_encoded,  # Use integer-encoded labels
            validation_data=(x_test, y_test_encoded),  # Use integer-encoded labels
            batch_size=args.batch_size,
            epochs=args.epochs,
            callbacks=[csv_logger]
        )

        loss, accuracy = model.evaluate(x_test, y_test_encoded, batch_size=args.batch_size, verbose=False)

        print("test loss:", str(loss))
        print("test accuracy", str(accuracy))

        predictions = np.argmax(model.predict(x_test, batch_size=args.batch_size), axis=1)

        # Calculate the confusion matrix
        conf_matrix = confusion_matrix(y_test_encoded, predictions)

        # Print or use the confusion matrix as needed
        print('Confusion Matrix:')
        print(conf_matrix)

        with open(log_file_path, mode='a') as f:
            f.write("integer encoding")
            f.write(f"df: {model_info['name']}\n")
            f.write(f"test loss: {loss}\n")
            f.write(f"test accuracy: {accuracy}\n")
            f.write(f'Confusion Matrix:\n{conf_matrix}\n')
            f.write("-------------------------------\n")


if __name__ == '__main__':
    train()