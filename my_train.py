# -*- coding: utf-8 -*-
""" 
@Time    : 2021/12/16 9:47
@Author  : Chen_Sir
@FileName: train.py
@SoftWare: PyCharm
"""

from keras.callbacks import CSVLogger
from keras.layers import Dense, concatenate
from keras.models import Model
from keras.optimizers import Adam
#from get_train_test import get_train_test
from get_dataset import get_df_1
from get_dataset import get_df_2
#from CBGRU.TF_IDF import get_df_bow
from Parser import parameter_parser
#from models.cnn import get_cnn
#from keras.utils.vis_utils import plot_model
#from models.gru import get_gru

import numpy as np
from sklearn.metrics import confusion_matrix
from keras import layers
from sklearn.feature_extraction import DictVectorizer
from BoW import get_df_bow
from TF_IDF import get_df_tfidf

import os
from keras.layers import Flatten

from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, Input, Dropout
from keras.models import Sequential
from keras.models import Model



import numpy as np
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
num_classes = 2
args = parameter_parser()


def get_train_test(dataset):
    vectors = np.stack(dataset.iloc[:, 0].values)
    labels = dataset.iloc[:, 1].values
    positive_index = np.where(labels == 1)[0][:1988]
    print(len(positive_index))
    negative_index = np.where(labels == 0)[0][:]
    print(len(negative_index))
    undersampled_negative_idxs = np.random.choice(negative_index, len(positive_index), replace=False)
    resampled_idxs = np.concatenate([positive_index, undersampled_negative_idxs])
    x_train, x_test, y_train, y_test = train_test_split(vectors[resampled_idxs], labels[resampled_idxs],
                                                        test_size=0.2, stratify=labels[resampled_idxs])
    return x_train, x_test, to_categorical(y_train), to_categorical(y_test)





# Function to define the model architecture
def ClassiFilerNet(INPUT_SIZE, TIME_STEPS, model_name):
    model = Sequential()
    if model_name in ['Word2Vec', 'FastText']:
        batch_input_shape = (None,1,100, 300)  # Adjusted for 2D input
    elif model_name == 'BoW':
        batch_input_shape = (None,1, 37, 1)  # Adjusted for 2D input
    elif model_name == 'TF-IDF':
        batch_input_shape = (None,1, 300, 1)  # Adjusted for 2D input
    else:
        batch_input_shape = (None, *INPUT_SIZE)

    model.add(
        Convolution2D(
            filters=32,
            kernel_size=(3, 3),  # Adjusted for 2D kernel, tells us the size of this filter. So, it's a 3x3 filter.
            padding="same",
            input_shape=batch_input_shape[1:],  # Remove the batch dimension
            name='cnn_input'
        )
    )
    #Regarding the kernel size (the (3, 3) part), it determines how big the filter is that scans over our input data. Smaller filters look for small features, and larger filters look for larger features. A (3, 3) filter is a common choice because it's small enough to capture fine details while also being computationally efficient.
    model.add(Activation('relu'))
    model.add(MaxPooling2D(
        pool_size=(2, 2),
        strides=(2, 2),
        padding='same',
    ))
    model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))  # Adjusted for 2D kernel
    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
    model.add(Flatten())
    model.add(Dropout(0.5))  # Add dropout layer with dropout rate 0.5
    model.add(Dense(300, activation='relu', name='dense_fc1'))
    model.add(Dense(300, activation='relu', name='dense_fc2'))
    model.add(Dense(num_classes, activation='softmax', name='dense_output'))  # Change activation to softmax

    return model





def train():
    # Extract dataset name without file extension
    dataset_name = os.path.splitext(os.path.basename(args.dataset))[0]
    
    # Create log directory based on dataset name
    log_directory = os.path.join("log", f"{dataset_name}_logs")
    os.makedirs(log_directory, exist_ok=True)
    
    train_models = [
        {'name': 'Word2Vec', 'get_data_function': get_df_1},
       {'name': 'FastText', 'get_data_function': get_df_2},
       {'name': 'BoW', 'get_data_function': get_df_bow},
    {'name': 'TF-IDF', 'get_data_function': get_df_tfidf}
    ]
    
    for model_info in train_models:
        print(f"Training for {model_info['name']}")
        
        df, _ = model_info['get_data_function']()
        x_train, x_test, y_train, y_test = get_train_test(df)
        
        print("Shape of x_train:", x_train.shape)
        # Reshape input data for CNN model
        # Reshape input data for CNN model
        if model_info['name'] == 'TF-IDF':
            x_train = x_train.reshape(-1, 1, 300, 1)#133, 105
            x_test = x_test.reshape(-1, 1, 300, 1)##133, 105
        elif model_info['name'] == 'BoW':
            x_train = x_train.reshape((-1, 1, 37, 1))#37,33,36
            x_test = x_test.reshape((-1, 1, 37, 1))#37
        else:
            x_train = x_train.reshape(-1, 1, 100, 300)#133
            x_test = x_test.reshape(-1, 1, 100, 300)#133
            # Construct the model
            #model = ClassiFilerNet(x_train.shape[1], x_train.shape[2], model_info['name'])

        
            # Construct the model
        model = ClassiFilerNet(x_train.shape[1], x_train.shape[2], model_info['name'])


        
       
        adam = Adam(learning_rate=args.lr)
        model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

        # Define the log file path based on the dataset name
        log_file_path = os.path.join(log_directory, f'{model_info["name"]}_log.txt')
        csv_logger = CSVLogger(log_file_path, append=True, separator=',')

        history = model.fit(
            x=x_train,
            y=y_train,
            validation_data=(x_test, y_test),
            batch_size=args.batch_size,
            epochs=args.epochs,
            callbacks=[csv_logger]
        )

        loss, accuracy = model.evaluate(x_test, y_test, batch_size=y_test.shape[0], verbose=False)

        print("test loss:", str(loss))
        print("test accuracy", str(accuracy))

        predictions = (model.predict(x_test, batch_size=y_test.shape[0])).round()

        tn, fp, fn, tp = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1)).ravel()
        print('False positive rate(FP): ', fp / (fp + tn))
        print('False negative rate(FN): ', fn / (fn + tp))
        recall = tp / (tp + fn)
        print('Recall: ', recall)
        precision = tp / (tp + fp)
        print('Precision: ', precision)
        print('F1 score: ', (2 * precision * recall) / (precision + recall))

        with open(log_file_path, mode='a') as f:
            f.write(f"df: {model_info['name']}\n")
            f.write(f"test loss: {loss}\n")
            f.write(f"test accuracy: {accuracy}\n")
            f.write(f'False positive rate(FP): {fp / (fp + tn)}\n')
            f.write(f'False negative rate(FN): {fn / (fn + tp)}\n')
            f.write(f'Recall: {recall}\n')
            f.write(f'Precision: {precision}\n')
            f.write(f'F1 score: {(2 * precision * recall) / (precision + recall)}\n')
            f.write("-------------------------------\n")


if __name__ == '__main__':
    train()
