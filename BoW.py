import os
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from Parser import parameter_parser
from collections import Counter
import numpy as np

args = parameter_parser()
custom_stop_words = ["contract", "function", "ethereum", "address", "uint256"]  # Add your custom stop words here



def parse_file(filename):
    with open(filename, "r", encoding="utf8") as file:
        fragment = []
        fragment_val = 0
        for line in file:
            stripped = line.strip()
            if not stripped:
                continue
            if "-" * 33 in line and fragment:
                yield " ".join(fragment), fragment_val
                fragment = []
            elif stripped.split()[0].isdigit():
                if fragment:
                    if stripped.isdigit():
                        fragment_val = int(stripped)
                    else:
                        fragment.append(stripped)
            else:
                fragment.append(stripped)
    return " ".join(fragment), fragment_val



def get_vectors_df_bow(filename, vector_length=300, custom_stop_words=None):
    fragments = []
    values = []
    count = 0

    # Create CountVectorizer with optional custom_stop_words
    vectorizer = CountVectorizer(max_features=vector_length, stop_words=custom_stop_words, token_pattern=r"(?u)\b\w+\b")#r"(?u)\b\w+\b" defines a pattern that matches whole words in Unicode text. When used in the CountVectorizer, it ensures that the tokenizer extracts individual words from the input text while considering Unicode characters and boundaries.
    #CountVectorizer is used to convert text documents to a matrix of token counts.
    for fragment, val in parse_file(filename):
        count += 1
        print("Collecting fragments...", count, end="\r")
        fragments.append(" ".join(fragment))
        values.append(val)

    print()
    print("Training model...", end="\r")

    # Check if fragments contain any non-stop words
    non_stop_fragments = [frag for frag in fragments if any(word.isalpha() for word in frag.split())]
    #for word in frag.split(): Within each iteration, the fragment is split into words using the split() method, creating an iterable of words.
    #f any(word.isalpha():  checks if at least one of the words in the fragment contains alphabetic characters
    #word.isalpha() returns True if all characters in word are alphabetic.


    #non_stop_fragments contains only those fragments from the original fragments list that have at least one alphabetic character.
    if non_stop_fragments:
        # Fit the vectorizer on non-stop fragments
        vectorizer.fit_transform(non_stop_fragments)

        # Prepare vectors using the same structure as Word2Vec and FastText
        vectors = []
        count = 0
        for fragment, val in zip(fragments, values):
            count += 1
            print("Processing fragments...", count, end="\r")
            # Transform using the fitted vectorizer
            bow_vector = vectorizer.transform([fragment]).toarray().flatten()  #For each fragment, the function transforms it into a Bag-of-Words vector using the fitted CountVectorizer. 
            row = {"vector": bow_vector, "val": val}
            vectors.append(row)

        print()
        df = pd.DataFrame(vectors)

        # Reshape vectors to (n_samples, vector_length, 1)
        df['vector'] = df['vector']
    else:
        raise ValueError("All fragments seem to only contain stop words. Adjust preprocessing or CountVectorizer parameters.")

    return df


def get_df_bow():
    filename = args.dataset
    print("dataset:", filename)
    base = os.path.splitext(os.path.basename(filename))[0]
    vector_filename = base + "_BoW_fragment_vectors.pkl"
    vector_length = args.vector_dim

    if os.path.exists(vector_filename):
        df = pd.read_pickle(vector_filename)
    else:
        df = get_vectors_df_bow(filename, vector_length, custom_stop_words=None)
        df.to_pickle(vector_filename)

    return df, base






if __name__ == "__main__":
    custom_stop_words = ["contract", "function", "ethereum", "address", "uint256"]
    #df = get_vectors_df_bow("your_filename.txt", vector_length=300, custom_stop_words=custom_stop_words)
    #df, base = get_df_bow()
    #print("df:", df)

    df2, base = get_df_bow()
    print("df:", df2)
    
    print(df2.head())  # Print the first few rows of the DataFrame
    print(df2.shape)
    print (df2.columns)

#This column contains the BoW representations of fragments. Each entry in the 'vector' column is a list of lists, where the inner lists represent the BoW vector for each fragment. Each number within these inner lists corresponds to the frequency of a specific word or token in the fragment.
#val: This column contains the labels associated with each fragment. 